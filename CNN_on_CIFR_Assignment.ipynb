{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK3alCdFflQX"
   },
   "source": [
    "### CNN on CIFR Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:07.392316Z",
     "start_time": "2020-12-10T06:42:01.619788Z"
    },
    "id": "TLVcyNYKflQi"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:07.423311Z",
     "start_time": "2020-12-10T06:42:07.410314Z"
    },
    "id": "UmFqRM5tmmEs"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:07.454314Z",
     "start_time": "2020-12-10T06:42:07.439312Z"
    },
    "id": "CMvxSg1LmxvM"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:09.095530Z",
     "start_time": "2020-12-10T06:42:08.100163Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nv0eNG3nmz7H",
    "outputId": "2ce1339e-580b-4998-dd8f-d3a0fb1a7640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:10.625481Z",
     "start_time": "2020-12-10T06:42:10.608481Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWLyyHWam3Ql",
    "outputId": "cdd8d146-c329-441a-8ae6-26dfc7152698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:12.731350Z",
     "start_time": "2020-12-10T06:42:12.713352Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR1t6C3xm9ge",
    "outputId": "e05ea859-0769-40b8-f5f5-60fffedd44ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:13.422801Z",
     "start_time": "2020-12-10T06:42:13.363263Z"
    },
    "id": "NLL5Ozs3qL2r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "## Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:14.185653Z",
     "start_time": "2020-12-10T06:42:14.167621Z"
    },
    "id": "GR7k7gHdnASW"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:27.525311Z",
     "start_time": "2020-12-10T06:42:14.887112Z"
    },
    "id": "VidkzxmYnC3H"
   },
   "outputs": [],
   "source": [
    "num_filter = 16\n",
    "dropout_rate = 0.0\n",
    "l = 16\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (5,5), use_bias=False ,padding='same')(input)\n",
    "batch_norm = layers.BatchNormalization()(First_Conv2D)\n",
    "\n",
    "First_Block = denseblock(batch_norm, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:42:29.069993Z",
     "start_time": "2020-12-10T06:42:28.688384Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrLLFZTRnLw8",
    "outputId": "ef948de5-a3dc-4b75-a1b9-a543568fd7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   1200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 8)    3200        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 24)   96          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 8)    4800        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 32)   0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 8)    6400        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 40)   0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 40)   160         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 40)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 8)    8000        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 48)   0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 8)    9600        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 56)   0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 56)   224         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 56)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 8)    11200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 64)   0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 8)    12800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 72)   0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 72)   288         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 72)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 8)    14400       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 80)   0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 80)   320         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 8)    16000       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 88)   0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 88)   352         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 88)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 8)    17600       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 96)   0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 96)   384         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 8)    19200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 104)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 104)  416         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 104)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 8)    20800       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 112)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 112)  448         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 112)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 8)    22400       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 120)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 120)  480         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 120)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 8)    24000       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 128)  0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 8)    25600       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 136)  0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 136)  544         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 136)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 8)    27200       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 144)  0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 144)  576         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 144)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 8)    1152        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 8)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 8)    32          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 8)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 8)    1600        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 16)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 16)   64          concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 16)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 8)    3200        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 24)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 24)   96          concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 24)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 8)    4800        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 32)   0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 8)    6400        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 40)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 40)   160         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 40)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 8)    8000        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 48)   0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 48)   192         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 48)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 8)    9600        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 56)   0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 56)   224         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 56)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 8)    11200       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 64)   0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 64)   256         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 8)    12800       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 72)   0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 72)   288         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 72)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 8)    14400       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 80)   0           concatenate_23[0][0]             \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 80)   320         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 80)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 8)    16000       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 16, 16, 88)   0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 88)   352         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 88)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 8)    17600       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 96)   0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 96)   384         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 8)    19200       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 104)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 104)  416         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 104)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 8)    20800       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 112)  0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 112)  448         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 112)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 8)    22400       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 120)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 120)  480         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 120)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 8)    24000       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 128)  0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 128)  512         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 8)    25600       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 136)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 136)  544         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 136)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 8)    1088        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 8)      0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 8)      32          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 8)      0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 8)      1600        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 16)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 16)     64          concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 16)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 8)      3200        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 24)     0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 24)     96          concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 24)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 8)      4800        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 32)     0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 32)     128         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 32)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 8)      6400        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 40)     0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 40)     160         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 40)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 8)      8000        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 48)     0           concatenate_35[0][0]             \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 48)     192         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 48)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 8)      9600        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 8, 8, 56)     0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 56)     224         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 56)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 8)      11200       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 8, 8, 64)     0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 64)     256         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 64)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 8)      12800       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 8, 8, 72)     0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 72)     288         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 72)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 8)      14400       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 80)     0           concatenate_39[0][0]             \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 80)     320         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 8)      16000       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 8, 8, 88)     0           concatenate_40[0][0]             \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 88)     352         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 88)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 8)      17600       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 8, 8, 96)     0           concatenate_41[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 96)     384         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 96)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 8)      19200       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 104)    0           concatenate_42[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 104)    416         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 104)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 8)      20800       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 112)    0           concatenate_43[0][0]             \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 112)    448         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 112)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 8)      22400       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 120)    0           concatenate_44[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 120)    480         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 120)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 8)      24000       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 128)    0           concatenate_45[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 128)    512         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 8)      25600       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 136)    0           concatenate_46[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 136)    544         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 8, 8, 136)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 8)      1088        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 8)      0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 8)      32          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 8)      0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 8)      1600        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 4, 4, 16)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 16)     64          concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 16)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 8)      3200        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 4, 4, 24)     0           concatenate_48[0][0]             \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 24)     96          concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 24)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 8)      4800        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 4, 4, 32)     0           concatenate_49[0][0]             \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 32)     128         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 32)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 4, 4, 8)      6400        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 4, 4, 40)     0           concatenate_50[0][0]             \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 40)     160         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 4, 4, 40)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 4, 4, 8)      8000        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 4, 4, 48)     0           concatenate_51[0][0]             \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 48)     192         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 4, 4, 48)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 4, 4, 8)      9600        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 4, 4, 56)     0           concatenate_52[0][0]             \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 4, 4, 56)     224         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 4, 4, 56)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 4, 4, 8)      11200       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 4, 4, 64)     0           concatenate_53[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4, 4, 64)     256         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 4, 4, 64)     0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 4, 4, 8)      12800       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 4, 4, 72)     0           concatenate_54[0][0]             \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 4, 4, 72)     288         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 4, 4, 72)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 4, 4, 8)      14400       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 4, 4, 80)     0           concatenate_55[0][0]             \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 4, 4, 80)     320         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 80)     0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 4, 4, 8)      16000       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 4, 4, 88)     0           concatenate_56[0][0]             \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 4, 4, 88)     352         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 88)     0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 4, 4, 8)      17600       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 4, 4, 96)     0           concatenate_57[0][0]             \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 96)     384         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 4, 4, 96)     0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 4, 4, 8)      19200       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 4, 4, 104)    0           concatenate_58[0][0]             \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 104)    416         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 104)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 4, 4, 8)      20800       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 4, 4, 112)    0           concatenate_59[0][0]             \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4, 4, 112)    448         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 112)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 8)      22400       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 4, 4, 120)    0           concatenate_60[0][0]             \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 4, 4, 120)    480         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 120)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 8)      24000       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 4, 4, 128)    0           concatenate_61[0][0]             \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 4, 4, 128)    512         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 128)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 8)      25600       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 4, 4, 136)    0           concatenate_62[0][0]             \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 4, 4, 136)    544         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 4, 4, 136)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 136)    0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 544)          0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           5450        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 926,170\n",
      "Trainable params: 916,074\n",
      "Non-trainable params: 10,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:43:00.939335Z",
     "start_time": "2020-12-10T06:43:00.903283Z"
    },
    "id": "Jj-ptF6YnTtb"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qwil8kcdnYjM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "model_callback = ModelCheckpoint('model.hdf5',save_best_only=True,monitor='val_accuracy',verbose=1,save_weights_only=True)\n",
    "csv_callback = CSVLogger('training.csv',separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wq0glUqIn0Gd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen_train = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,width_shift_range=0.1,height_shift_range=0.1,rotation_range=20,shear_range=20,horizontal_flip=True,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lZn_5UWiGwJo"
   },
   "outputs": [],
   "source": [
    "datagen_train.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "haq8scaqHzjG"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/62948249/feature-wise-center-in-imagedatagenerator\n",
    "datagen_test = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True)\n",
    "datagen_test.mean = datagen_train.mean\n",
    "datagen_test.std = datagen_train.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb79fI0eoGA4"
   },
   "source": [
    "<h3>Fitting the model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMTQs8Dr_Q5k"
   },
   "source": [
    "<h3>30 epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnaJekpZo2Yr",
    "outputId": "efb7ac0f-eb38-42bd-cd87-ab444bf07f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.6890 - accuracy: 0.3772\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.42900, saving model to model.hdf5\n",
      "390/390 [==============================] - 68s 175ms/step - loss: 1.6890 - accuracy: 0.3772 - val_loss: 1.5983 - val_accuracy: 0.4290\n",
      "Epoch 2/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.3781 - accuracy: 0.4982\n",
      "Epoch 00002: val_accuracy improved from 0.42900 to 0.49380, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 1.3781 - accuracy: 0.4982 - val_loss: 1.5834 - val_accuracy: 0.4938\n",
      "Epoch 3/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.2190 - accuracy: 0.5605\n",
      "Epoch 00003: val_accuracy improved from 0.49380 to 0.60270, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 1.2190 - accuracy: 0.5605 - val_loss: 1.1272 - val_accuracy: 0.6027\n",
      "Epoch 4/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.6081\n",
      "Epoch 00004: val_accuracy improved from 0.60270 to 0.63970, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 1.0972 - accuracy: 0.6081 - val_loss: 1.0537 - val_accuracy: 0.6397\n",
      "Epoch 5/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0051 - accuracy: 0.6441\n",
      "Epoch 00005: val_accuracy improved from 0.63970 to 0.64240, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 1.0051 - accuracy: 0.6441 - val_loss: 1.0642 - val_accuracy: 0.6424\n",
      "Epoch 6/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9388 - accuracy: 0.6665\n",
      "Epoch 00006: val_accuracy improved from 0.64240 to 0.67890, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.9388 - accuracy: 0.6665 - val_loss: 0.9134 - val_accuracy: 0.6789\n",
      "Epoch 7/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8824 - accuracy: 0.6896\n",
      "Epoch 00007: val_accuracy did not improve from 0.67890\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.8824 - accuracy: 0.6896 - val_loss: 1.0476 - val_accuracy: 0.6587\n",
      "Epoch 8/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8402 - accuracy: 0.7054\n",
      "Epoch 00008: val_accuracy did not improve from 0.67890\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.8402 - accuracy: 0.7054 - val_loss: 0.9711 - val_accuracy: 0.6779\n",
      "Epoch 9/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8037 - accuracy: 0.7172\n",
      "Epoch 00009: val_accuracy improved from 0.67890 to 0.69320, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.8037 - accuracy: 0.7172 - val_loss: 0.9298 - val_accuracy: 0.6932\n",
      "Epoch 10/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.7295\n",
      "Epoch 00010: val_accuracy improved from 0.69320 to 0.72600, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.7710 - accuracy: 0.7295 - val_loss: 0.8208 - val_accuracy: 0.7260\n",
      "Epoch 11/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7400 - accuracy: 0.7400\n",
      "Epoch 00011: val_accuracy did not improve from 0.72600\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.7400 - accuracy: 0.7400 - val_loss: 0.9112 - val_accuracy: 0.7013\n",
      "Epoch 12/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7206 - accuracy: 0.7494\n",
      "Epoch 00012: val_accuracy improved from 0.72600 to 0.73820, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.7206 - accuracy: 0.7494 - val_loss: 0.7840 - val_accuracy: 0.7382\n",
      "Epoch 13/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7585\n",
      "Epoch 00013: val_accuracy improved from 0.73820 to 0.76100, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.6888 - accuracy: 0.7585 - val_loss: 0.7110 - val_accuracy: 0.7610\n",
      "Epoch 14/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.7664\n",
      "Epoch 00014: val_accuracy did not improve from 0.76100\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.6695 - accuracy: 0.7664 - val_loss: 1.0046 - val_accuracy: 0.7023\n",
      "Epoch 15/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.7756\n",
      "Epoch 00015: val_accuracy improved from 0.76100 to 0.76610, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.6468 - accuracy: 0.7756 - val_loss: 0.7101 - val_accuracy: 0.7661\n",
      "Epoch 16/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7796\n",
      "Epoch 00016: val_accuracy improved from 0.76610 to 0.76730, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.6305 - accuracy: 0.7796 - val_loss: 0.7083 - val_accuracy: 0.7673\n",
      "Epoch 17/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.7871\n",
      "Epoch 00017: val_accuracy improved from 0.76730 to 0.78650, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.6167 - accuracy: 0.7871 - val_loss: 0.6225 - val_accuracy: 0.7865\n",
      "Epoch 18/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5973 - accuracy: 0.7909\n",
      "Epoch 00018: val_accuracy did not improve from 0.78650\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5973 - accuracy: 0.7909 - val_loss: 0.7975 - val_accuracy: 0.7513\n",
      "Epoch 19/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.7958\n",
      "Epoch 00019: val_accuracy did not improve from 0.78650\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5825 - accuracy: 0.7958 - val_loss: 0.8006 - val_accuracy: 0.7458\n",
      "Epoch 20/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.8011\n",
      "Epoch 00020: val_accuracy improved from 0.78650 to 0.78960, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.5751 - accuracy: 0.8011 - val_loss: 0.6456 - val_accuracy: 0.7896\n",
      "Epoch 21/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.8066\n",
      "Epoch 00021: val_accuracy improved from 0.78960 to 0.82230, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.5525 - accuracy: 0.8066 - val_loss: 0.5258 - val_accuracy: 0.8223\n",
      "Epoch 22/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.8073\n",
      "Epoch 00022: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5518 - accuracy: 0.8073 - val_loss: 0.7744 - val_accuracy: 0.7580\n",
      "Epoch 23/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.8129\n",
      "Epoch 00023: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5357 - accuracy: 0.8129 - val_loss: 0.6988 - val_accuracy: 0.7746\n",
      "Epoch 24/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.8152\n",
      "Epoch 00024: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5261 - accuracy: 0.8152 - val_loss: 0.7780 - val_accuracy: 0.7644\n",
      "Epoch 25/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.8223\n",
      "Epoch 00025: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5102 - accuracy: 0.8223 - val_loss: 0.6648 - val_accuracy: 0.7819\n",
      "Epoch 26/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.8246\n",
      "Epoch 00026: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.5055 - accuracy: 0.8246 - val_loss: 0.6283 - val_accuracy: 0.7987\n",
      "Epoch 27/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.8275\n",
      "Epoch 00027: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4951 - accuracy: 0.8275 - val_loss: 0.6549 - val_accuracy: 0.7929\n",
      "Epoch 28/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8297\n",
      "Epoch 00028: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4883 - accuracy: 0.8297 - val_loss: 0.6198 - val_accuracy: 0.7984\n",
      "Epoch 29/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8337\n",
      "Epoch 00029: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4762 - accuracy: 0.8337 - val_loss: 0.6599 - val_accuracy: 0.7928\n",
      "Epoch 30/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.8377\n",
      "Epoch 00030: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4705 - accuracy: 0.8377 - val_loss: 0.5727 - val_accuracy: 0.8159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f09418f8d68>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=30,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqYMTlrWhNGc"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqO-khrrU52y",
    "outputId": "43717958-d55d-4b5a-8edc-0da590130190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.8374\n",
      "Epoch 00001: val_accuracy did not improve from 0.82230\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.4646 - accuracy: 0.8374 - val_loss: 0.5570 - val_accuracy: 0.8165\n",
      "Epoch 2/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8425\n",
      "Epoch 00002: val_accuracy improved from 0.82230 to 0.84070, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4521 - accuracy: 0.8425 - val_loss: 0.4850 - val_accuracy: 0.8407\n",
      "Epoch 3/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8444\n",
      "Epoch 00003: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4493 - accuracy: 0.8444 - val_loss: 0.5070 - val_accuracy: 0.8315\n",
      "Epoch 4/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8446\n",
      "Epoch 00004: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4453 - accuracy: 0.8446 - val_loss: 0.5751 - val_accuracy: 0.8195\n",
      "Epoch 5/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.8496\n",
      "Epoch 00005: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.4322 - accuracy: 0.8496 - val_loss: 0.5171 - val_accuracy: 0.8353\n",
      "Epoch 6/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.8501\n",
      "Epoch 00006: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.4320 - accuracy: 0.8501 - val_loss: 0.7729 - val_accuracy: 0.7710\n",
      "Epoch 7/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8518\n",
      "Epoch 00007: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.4252 - accuracy: 0.8518 - val_loss: 0.5269 - val_accuracy: 0.8278\n",
      "Epoch 8/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.8526\n",
      "Epoch 00008: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4209 - accuracy: 0.8526 - val_loss: 0.5349 - val_accuracy: 0.8282\n",
      "Epoch 9/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.8550\n",
      "Epoch 00009: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4145 - accuracy: 0.8550 - val_loss: 0.6693 - val_accuracy: 0.8006\n",
      "Epoch 10/10\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8601\n",
      "Epoch 00010: val_accuracy did not improve from 0.84070\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4028 - accuracy: 0.8601 - val_loss: 0.5721 - val_accuracy: 0.8164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f093e777fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=10,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86B_jdy2ZYwQ",
    "outputId": "d3f6d97c-93f4-46c9-977d-34655b4ca24a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8601\n",
      "Epoch 00001: val_accuracy improved from 0.84070 to 0.85550, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.4017 - accuracy: 0.8601 - val_loss: 0.4272 - val_accuracy: 0.8555\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3909 - accuracy: 0.8647\n",
      "Epoch 00002: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.3909 - accuracy: 0.8647 - val_loss: 0.5574 - val_accuracy: 0.8255\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8626\n",
      "Epoch 00003: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3891 - accuracy: 0.8626 - val_loss: 0.4666 - val_accuracy: 0.8490\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.8644\n",
      "Epoch 00004: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.3868 - accuracy: 0.8644 - val_loss: 0.4754 - val_accuracy: 0.8477\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8682\n",
      "Epoch 00005: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.3778 - accuracy: 0.8682 - val_loss: 0.5162 - val_accuracy: 0.8339\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.8688\n",
      "Epoch 00006: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.3723 - accuracy: 0.8688 - val_loss: 0.5013 - val_accuracy: 0.8420\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.8685\n",
      "Epoch 00007: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.3754 - accuracy: 0.8685 - val_loss: 0.6954 - val_accuracy: 0.7937\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.8713\n",
      "Epoch 00008: val_accuracy did not improve from 0.85550\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3675 - accuracy: 0.8713 - val_loss: 0.4729 - val_accuracy: 0.8488\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8718\n",
      "Epoch 00009: val_accuracy improved from 0.85550 to 0.86130, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3668 - accuracy: 0.8718 - val_loss: 0.4273 - val_accuracy: 0.8613\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8718\n",
      "Epoch 00010: val_accuracy improved from 0.86130 to 0.86470, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3620 - accuracy: 0.8718 - val_loss: 0.4069 - val_accuracy: 0.8647\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.8745\n",
      "Epoch 00011: val_accuracy did not improve from 0.86470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3595 - accuracy: 0.8745 - val_loss: 0.5190 - val_accuracy: 0.8390\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.8781\n",
      "Epoch 00012: val_accuracy did not improve from 0.86470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3512 - accuracy: 0.8781 - val_loss: 0.4485 - val_accuracy: 0.8540\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8785\n",
      "Epoch 00013: val_accuracy improved from 0.86470 to 0.87110, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.3490 - accuracy: 0.8785 - val_loss: 0.3933 - val_accuracy: 0.8711\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.8790\n",
      "Epoch 00014: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3471 - accuracy: 0.8790 - val_loss: 0.4305 - val_accuracy: 0.8591\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.8808\n",
      "Epoch 00015: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3403 - accuracy: 0.8808 - val_loss: 0.5262 - val_accuracy: 0.8317\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8820\n",
      "Epoch 00016: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3353 - accuracy: 0.8820 - val_loss: 0.5024 - val_accuracy: 0.8461\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.8811\n",
      "Epoch 00017: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3388 - accuracy: 0.8811 - val_loss: 0.4617 - val_accuracy: 0.8555\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8809\n",
      "Epoch 00018: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3358 - accuracy: 0.8809 - val_loss: 0.4403 - val_accuracy: 0.8570\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.8847\n",
      "Epoch 00019: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.3274 - accuracy: 0.8847 - val_loss: 0.5368 - val_accuracy: 0.8396\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.8856\n",
      "Epoch 00020: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3246 - accuracy: 0.8856 - val_loss: 0.5249 - val_accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f093e6c4a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=20,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2jXC5gycxN9",
    "outputId": "4f97e7f8-8790-42fd-b26e-8effddce2cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8850\n",
      "Epoch 00001: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.3248 - accuracy: 0.8850 - val_loss: 0.4247 - val_accuracy: 0.8636\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.8882\n",
      "Epoch 00002: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3205 - accuracy: 0.8882 - val_loss: 0.7052 - val_accuracy: 0.8078\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8878\n",
      "Epoch 00003: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3170 - accuracy: 0.8878 - val_loss: 0.4119 - val_accuracy: 0.8660\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8902\n",
      "Epoch 00004: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.3160 - accuracy: 0.8902 - val_loss: 0.4325 - val_accuracy: 0.8548\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8920\n",
      "Epoch 00005: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3098 - accuracy: 0.8920 - val_loss: 0.5310 - val_accuracy: 0.8382\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8915\n",
      "Epoch 00006: val_accuracy did not improve from 0.87110\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.3093 - accuracy: 0.8915 - val_loss: 0.4353 - val_accuracy: 0.8670\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.8911\n",
      "Epoch 00007: val_accuracy improved from 0.87110 to 0.87470, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3085 - accuracy: 0.8911 - val_loss: 0.3930 - val_accuracy: 0.8747\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.8952\n",
      "Epoch 00008: val_accuracy did not improve from 0.87470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2983 - accuracy: 0.8952 - val_loss: 0.4660 - val_accuracy: 0.8562\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.8925\n",
      "Epoch 00009: val_accuracy did not improve from 0.87470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.3035 - accuracy: 0.8925 - val_loss: 0.4247 - val_accuracy: 0.8640\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8970\n",
      "Epoch 00010: val_accuracy did not improve from 0.87470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2951 - accuracy: 0.8970 - val_loss: 0.4103 - val_accuracy: 0.8709\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8971\n",
      "Epoch 00011: val_accuracy did not improve from 0.87470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2938 - accuracy: 0.8971 - val_loss: 0.4292 - val_accuracy: 0.8657\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8979\n",
      "Epoch 00012: val_accuracy did not improve from 0.87470\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2930 - accuracy: 0.8979 - val_loss: 0.4292 - val_accuracy: 0.8615\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.8988\n",
      "Epoch 00013: val_accuracy improved from 0.87470 to 0.88400, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2917 - accuracy: 0.8988 - val_loss: 0.3602 - val_accuracy: 0.8840\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8971\n",
      "Epoch 00014: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2903 - accuracy: 0.8971 - val_loss: 0.4315 - val_accuracy: 0.8660\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8982\n",
      "Epoch 00015: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2913 - accuracy: 0.8982 - val_loss: 0.4090 - val_accuracy: 0.8684\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9006\n",
      "Epoch 00016: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2833 - accuracy: 0.9006 - val_loss: 0.3811 - val_accuracy: 0.8777\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9008\n",
      "Epoch 00017: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2850 - accuracy: 0.9008 - val_loss: 0.4404 - val_accuracy: 0.8629\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9015\n",
      "Epoch 00018: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2808 - accuracy: 0.9015 - val_loss: 0.4210 - val_accuracy: 0.8701\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9010\n",
      "Epoch 00019: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2787 - accuracy: 0.9010 - val_loss: 0.5049 - val_accuracy: 0.8526\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.9049\n",
      "Epoch 00020: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2726 - accuracy: 0.9049 - val_loss: 0.4276 - val_accuracy: 0.8611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f093e695f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=20,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nynLluMejDii",
    "outputId": "f71f195c-d651-445a-870d-dfa5a4cc0790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9023\n",
      "Epoch 00001: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2749 - accuracy: 0.9023 - val_loss: 0.4072 - val_accuracy: 0.8712\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9030\n",
      "Epoch 00002: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2737 - accuracy: 0.9030 - val_loss: 0.4531 - val_accuracy: 0.8642\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.9039\n",
      "Epoch 00003: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2728 - accuracy: 0.9039 - val_loss: 0.3731 - val_accuracy: 0.8832\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9067\n",
      "Epoch 00004: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2655 - accuracy: 0.9067 - val_loss: 0.4092 - val_accuracy: 0.8771\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.9067\n",
      "Epoch 00005: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2644 - accuracy: 0.9067 - val_loss: 0.4301 - val_accuracy: 0.8726\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9068\n",
      "Epoch 00006: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2622 - accuracy: 0.9068 - val_loss: 0.4412 - val_accuracy: 0.8650\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9074\n",
      "Epoch 00007: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2635 - accuracy: 0.9074 - val_loss: 0.4486 - val_accuracy: 0.8652\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9091\n",
      "Epoch 00008: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2580 - accuracy: 0.9091 - val_loss: 0.4497 - val_accuracy: 0.8662\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9073\n",
      "Epoch 00009: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2623 - accuracy: 0.9073 - val_loss: 0.3767 - val_accuracy: 0.8787\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9097\n",
      "Epoch 00010: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2570 - accuracy: 0.9097 - val_loss: 0.5260 - val_accuracy: 0.8461\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9100\n",
      "Epoch 00011: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2525 - accuracy: 0.9100 - val_loss: 0.3715 - val_accuracy: 0.8832\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9092\n",
      "Epoch 00012: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2556 - accuracy: 0.9092 - val_loss: 0.4543 - val_accuracy: 0.8637\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9106\n",
      "Epoch 00013: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2543 - accuracy: 0.9106 - val_loss: 0.4054 - val_accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9111\n",
      "Epoch 00014: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2501 - accuracy: 0.9111 - val_loss: 0.3992 - val_accuracy: 0.8803\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9134\n",
      "Epoch 00015: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2484 - accuracy: 0.9134 - val_loss: 0.4080 - val_accuracy: 0.8762\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9124\n",
      "Epoch 00016: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2498 - accuracy: 0.9124 - val_loss: 0.4083 - val_accuracy: 0.8769\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9153\n",
      "Epoch 00017: val_accuracy did not improve from 0.88400\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2397 - accuracy: 0.9153 - val_loss: 0.4899 - val_accuracy: 0.8632\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9141\n",
      "Epoch 00018: val_accuracy improved from 0.88400 to 0.88980, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2444 - accuracy: 0.9141 - val_loss: 0.3521 - val_accuracy: 0.8898\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9150\n",
      "Epoch 00019: val_accuracy improved from 0.88980 to 0.89040, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2423 - accuracy: 0.9150 - val_loss: 0.3496 - val_accuracy: 0.8904\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9164\n",
      "Epoch 00020: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2397 - accuracy: 0.9164 - val_loss: 0.3581 - val_accuracy: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f093e6b8a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=20,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gflp0von5WL",
    "outputId": "c67cb3a8-231f-4b90-f00d-73a09830ff02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9165\n",
      "Epoch 00001: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2367 - accuracy: 0.9165 - val_loss: 0.3914 - val_accuracy: 0.8793\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9168\n",
      "Epoch 00002: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2363 - accuracy: 0.9168 - val_loss: 0.5208 - val_accuracy: 0.8497\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9168\n",
      "Epoch 00003: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2346 - accuracy: 0.9168 - val_loss: 0.3894 - val_accuracy: 0.8843\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2359 - accuracy: 0.9173\n",
      "Epoch 00004: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2359 - accuracy: 0.9173 - val_loss: 0.4253 - val_accuracy: 0.8751\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9190\n",
      "Epoch 00005: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2325 - accuracy: 0.9190 - val_loss: 0.3837 - val_accuracy: 0.8840\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9185\n",
      "Epoch 00006: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2318 - accuracy: 0.9185 - val_loss: 0.4891 - val_accuracy: 0.8584\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9193\n",
      "Epoch 00007: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2289 - accuracy: 0.9193 - val_loss: 0.3629 - val_accuracy: 0.8900\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9185\n",
      "Epoch 00008: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2301 - accuracy: 0.9185 - val_loss: 0.3879 - val_accuracy: 0.8820\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.9190\n",
      "Epoch 00009: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2311 - accuracy: 0.9190 - val_loss: 0.4294 - val_accuracy: 0.8706\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9202\n",
      "Epoch 00010: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2259 - accuracy: 0.9202 - val_loss: 0.4096 - val_accuracy: 0.8758\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9191\n",
      "Epoch 00011: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2260 - accuracy: 0.9191 - val_loss: 0.3870 - val_accuracy: 0.8810\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9207\n",
      "Epoch 00012: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.2269 - accuracy: 0.9207 - val_loss: 0.3665 - val_accuracy: 0.8882\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9221\n",
      "Epoch 00013: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2217 - accuracy: 0.9221 - val_loss: 0.3912 - val_accuracy: 0.8866\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9201\n",
      "Epoch 00014: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2239 - accuracy: 0.9201 - val_loss: 0.3673 - val_accuracy: 0.8873\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9218\n",
      "Epoch 00015: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2220 - accuracy: 0.9218 - val_loss: 0.3976 - val_accuracy: 0.8780\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.9216\n",
      "Epoch 00016: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.2209 - accuracy: 0.9216 - val_loss: 0.3662 - val_accuracy: 0.8868\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9229\n",
      "Epoch 00017: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2212 - accuracy: 0.9229 - val_loss: 0.3872 - val_accuracy: 0.8826\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9222\n",
      "Epoch 00018: val_accuracy did not improve from 0.89040\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2191 - accuracy: 0.9222 - val_loss: 0.3754 - val_accuracy: 0.8890\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9242\n",
      "Epoch 00019: val_accuracy improved from 0.89040 to 0.89440, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2184 - accuracy: 0.9242 - val_loss: 0.3322 - val_accuracy: 0.8944\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9242\n",
      "Epoch 00020: val_accuracy did not improve from 0.89440\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2136 - accuracy: 0.9242 - val_loss: 0.5573 - val_accuracy: 0.8457\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9254\n",
      "Epoch 00021: val_accuracy did not improve from 0.89440\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2106 - accuracy: 0.9254 - val_loss: 0.4485 - val_accuracy: 0.8731\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9262\n",
      "Epoch 00022: val_accuracy did not improve from 0.89440\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2102 - accuracy: 0.9262 - val_loss: 0.4064 - val_accuracy: 0.8813\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9250\n",
      "Epoch 00023: val_accuracy did not improve from 0.89440\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2103 - accuracy: 0.9250 - val_loss: 0.4044 - val_accuracy: 0.8833\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9273\n",
      "Epoch 00024: val_accuracy improved from 0.89440 to 0.89780, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2075 - accuracy: 0.9273 - val_loss: 0.3360 - val_accuracy: 0.8978\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9252\n",
      "Epoch 00025: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2092 - accuracy: 0.9252 - val_loss: 0.5476 - val_accuracy: 0.8531\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9263\n",
      "Epoch 00026: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2075 - accuracy: 0.9263 - val_loss: 0.4998 - val_accuracy: 0.8627\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9279\n",
      "Epoch 00027: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2041 - accuracy: 0.9279 - val_loss: 0.4423 - val_accuracy: 0.8751\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9293\n",
      "Epoch 00028: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2019 - accuracy: 0.9293 - val_loss: 0.4173 - val_accuracy: 0.8776\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9277\n",
      "Epoch 00029: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.2025 - accuracy: 0.9277 - val_loss: 0.4120 - val_accuracy: 0.8769\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9281\n",
      "Epoch 00030: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2021 - accuracy: 0.9281 - val_loss: 0.3829 - val_accuracy: 0.8876\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9281\n",
      "Epoch 00031: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2040 - accuracy: 0.9281 - val_loss: 0.4403 - val_accuracy: 0.8712\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9301\n",
      "Epoch 00032: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1990 - accuracy: 0.9301 - val_loss: 0.3837 - val_accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9296\n",
      "Epoch 00033: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2003 - accuracy: 0.9296 - val_loss: 0.4520 - val_accuracy: 0.8724\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9288\n",
      "Epoch 00034: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.2013 - accuracy: 0.9288 - val_loss: 0.4882 - val_accuracy: 0.8657\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9301\n",
      "Epoch 00035: val_accuracy did not improve from 0.89780\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1976 - accuracy: 0.9301 - val_loss: 0.4928 - val_accuracy: 0.8634\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1967 - accuracy: 0.9316\n",
      "Epoch 00036: val_accuracy improved from 0.89780 to 0.90080, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 168ms/step - loss: 0.1967 - accuracy: 0.9316 - val_loss: 0.3333 - val_accuracy: 0.9008\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9301\n",
      "Epoch 00037: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1976 - accuracy: 0.9301 - val_loss: 0.4151 - val_accuracy: 0.8798\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9306\n",
      "Epoch 00038: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1972 - accuracy: 0.9306 - val_loss: 0.3697 - val_accuracy: 0.8888\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9320\n",
      "Epoch 00039: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1946 - accuracy: 0.9320 - val_loss: 0.4111 - val_accuracy: 0.8799\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.9304\n",
      "Epoch 00040: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1945 - accuracy: 0.9304 - val_loss: 0.3791 - val_accuracy: 0.8891\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9303\n",
      "Epoch 00041: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1947 - accuracy: 0.9303 - val_loss: 0.3628 - val_accuracy: 0.8884\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9304\n",
      "Epoch 00042: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1934 - accuracy: 0.9304 - val_loss: 0.3673 - val_accuracy: 0.8909\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9312\n",
      "Epoch 00043: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1918 - accuracy: 0.9312 - val_loss: 0.4433 - val_accuracy: 0.8750\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9315\n",
      "Epoch 00044: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1946 - accuracy: 0.9315 - val_loss: 0.3463 - val_accuracy: 0.8950\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9334\n",
      "Epoch 00045: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1883 - accuracy: 0.9334 - val_loss: 0.3989 - val_accuracy: 0.8838\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9332\n",
      "Epoch 00046: val_accuracy improved from 0.90080 to 0.90330, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 168ms/step - loss: 0.1876 - accuracy: 0.9332 - val_loss: 0.3251 - val_accuracy: 0.9033\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9338\n",
      "Epoch 00047: val_accuracy did not improve from 0.90330\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1874 - accuracy: 0.9338 - val_loss: 0.3720 - val_accuracy: 0.8877\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9339\n",
      "Epoch 00048: val_accuracy did not improve from 0.90330\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1875 - accuracy: 0.9339 - val_loss: 0.4247 - val_accuracy: 0.8776\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9334\n",
      "Epoch 00049: val_accuracy did not improve from 0.90330\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1886 - accuracy: 0.9334 - val_loss: 0.3763 - val_accuracy: 0.8892\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9354\n",
      "Epoch 00050: val_accuracy did not improve from 0.90330\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1856 - accuracy: 0.9354 - val_loss: 0.4493 - val_accuracy: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f093e6686d8>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=50,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gBQ5ikhAAdl8"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.hdf5')\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001,epsilon=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59LALVz10go2",
    "outputId": "afbe50da-7e79-4eeb-ab2f-89327b2955ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9519\n",
      "Epoch 00001: val_accuracy improved from 0.90380 to 0.91010, saving model to model.hdf5\n",
      "390/390 [==============================] - 67s 171ms/step - loss: 0.1349 - accuracy: 0.9519 - val_loss: 0.3191 - val_accuracy: 0.9101\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9525\n",
      "Epoch 00002: val_accuracy improved from 0.91010 to 0.91170, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 168ms/step - loss: 0.1345 - accuracy: 0.9525 - val_loss: 0.3086 - val_accuracy: 0.9117\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9548\n",
      "Epoch 00003: val_accuracy did not improve from 0.91170\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1273 - accuracy: 0.9548 - val_loss: 0.3201 - val_accuracy: 0.9082\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9559\n",
      "Epoch 00004: val_accuracy improved from 0.91170 to 0.91180, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1264 - accuracy: 0.9559 - val_loss: 0.3141 - val_accuracy: 0.9118\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9555\n",
      "Epoch 00005: val_accuracy did not improve from 0.91180\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1257 - accuracy: 0.9555 - val_loss: 0.3257 - val_accuracy: 0.9094\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9574\n",
      "Epoch 00006: val_accuracy improved from 0.91180 to 0.91200, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1213 - accuracy: 0.9574 - val_loss: 0.3171 - val_accuracy: 0.9120\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9571\n",
      "Epoch 00007: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1225 - accuracy: 0.9571 - val_loss: 0.3246 - val_accuracy: 0.9108\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9569\n",
      "Epoch 00008: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1225 - accuracy: 0.9569 - val_loss: 0.3328 - val_accuracy: 0.9085\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9573\n",
      "Epoch 00009: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1220 - accuracy: 0.9573 - val_loss: 0.3289 - val_accuracy: 0.9100\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9573\n",
      "Epoch 00010: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1224 - accuracy: 0.9573 - val_loss: 0.3188 - val_accuracy: 0.9118\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9600\n",
      "Epoch 00011: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1153 - accuracy: 0.9600 - val_loss: 0.3270 - val_accuracy: 0.9107\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9588\n",
      "Epoch 00012: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1190 - accuracy: 0.9588 - val_loss: 0.3236 - val_accuracy: 0.9106\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9592\n",
      "Epoch 00013: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1177 - accuracy: 0.9592 - val_loss: 0.3292 - val_accuracy: 0.9104\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9592\n",
      "Epoch 00014: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1158 - accuracy: 0.9592 - val_loss: 0.3319 - val_accuracy: 0.9096\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9594\n",
      "Epoch 00015: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1152 - accuracy: 0.9594 - val_loss: 0.3316 - val_accuracy: 0.9110\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9582\n",
      "Epoch 00016: val_accuracy did not improve from 0.91200\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1158 - accuracy: 0.9582 - val_loss: 0.3371 - val_accuracy: 0.9083\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9587\n",
      "Epoch 00017: val_accuracy improved from 0.91200 to 0.91260, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1153 - accuracy: 0.9587 - val_loss: 0.3245 - val_accuracy: 0.9126\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9596\n",
      "Epoch 00018: val_accuracy did not improve from 0.91260\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1153 - accuracy: 0.9596 - val_loss: 0.3217 - val_accuracy: 0.9113\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9603\n",
      "Epoch 00019: val_accuracy did not improve from 0.91260\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1147 - accuracy: 0.9603 - val_loss: 0.3253 - val_accuracy: 0.9107\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9609\n",
      "Epoch 00020: val_accuracy improved from 0.91260 to 0.91390, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1110 - accuracy: 0.9609 - val_loss: 0.3201 - val_accuracy: 0.9139\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9606\n",
      "Epoch 00021: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1107 - accuracy: 0.9606 - val_loss: 0.3243 - val_accuracy: 0.9106\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9589\n",
      "Epoch 00022: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1144 - accuracy: 0.9589 - val_loss: 0.3288 - val_accuracy: 0.9112\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9594\n",
      "Epoch 00023: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1142 - accuracy: 0.9594 - val_loss: 0.3342 - val_accuracy: 0.9094\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9584\n",
      "Epoch 00024: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1165 - accuracy: 0.9584 - val_loss: 0.3251 - val_accuracy: 0.9104\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9588\n",
      "Epoch 00025: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1162 - accuracy: 0.9588 - val_loss: 0.3199 - val_accuracy: 0.9116\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9603\n",
      "Epoch 00026: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1144 - accuracy: 0.9603 - val_loss: 0.3286 - val_accuracy: 0.9109\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9602\n",
      "Epoch 00027: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1133 - accuracy: 0.9602 - val_loss: 0.3254 - val_accuracy: 0.9118\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9607\n",
      "Epoch 00028: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1119 - accuracy: 0.9607 - val_loss: 0.3262 - val_accuracy: 0.9113\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9608\n",
      "Epoch 00029: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1113 - accuracy: 0.9608 - val_loss: 0.3230 - val_accuracy: 0.9137\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9603\n",
      "Epoch 00030: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1117 - accuracy: 0.9603 - val_loss: 0.3241 - val_accuracy: 0.9116\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9613\n",
      "Epoch 00031: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1103 - accuracy: 0.9613 - val_loss: 0.3363 - val_accuracy: 0.9085\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9615\n",
      "Epoch 00032: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1100 - accuracy: 0.9615 - val_loss: 0.3293 - val_accuracy: 0.9134\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9613\n",
      "Epoch 00033: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1106 - accuracy: 0.9613 - val_loss: 0.3267 - val_accuracy: 0.9114\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9615\n",
      "Epoch 00034: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1107 - accuracy: 0.9615 - val_loss: 0.3373 - val_accuracy: 0.9096\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9616\n",
      "Epoch 00035: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1102 - accuracy: 0.9616 - val_loss: 0.3278 - val_accuracy: 0.9118\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9621\n",
      "Epoch 00036: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1096 - accuracy: 0.9621 - val_loss: 0.3275 - val_accuracy: 0.9108\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9612\n",
      "Epoch 00037: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1100 - accuracy: 0.9612 - val_loss: 0.3405 - val_accuracy: 0.9093\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9611\n",
      "Epoch 00038: val_accuracy did not improve from 0.91390\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1100 - accuracy: 0.9611 - val_loss: 0.3351 - val_accuracy: 0.9106\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9613\n",
      "Epoch 00039: val_accuracy improved from 0.91390 to 0.91590, saving model to model.hdf5\n",
      "390/390 [==============================] - 65s 167ms/step - loss: 0.1082 - accuracy: 0.9613 - val_loss: 0.3158 - val_accuracy: 0.9159\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9605\n",
      "Epoch 00040: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 65s 166ms/step - loss: 0.1114 - accuracy: 0.9605 - val_loss: 0.3350 - val_accuracy: 0.9099\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9621\n",
      "Epoch 00041: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1081 - accuracy: 0.9621 - val_loss: 0.3389 - val_accuracy: 0.9104\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9630\n",
      "Epoch 00042: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1062 - accuracy: 0.9630 - val_loss: 0.3468 - val_accuracy: 0.9080\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9629\n",
      "Epoch 00043: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1058 - accuracy: 0.9629 - val_loss: 0.3452 - val_accuracy: 0.9092\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9618\n",
      "Epoch 00044: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.1086 - accuracy: 0.9618 - val_loss: 0.3239 - val_accuracy: 0.9130\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9627\n",
      "Epoch 00045: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1061 - accuracy: 0.9627 - val_loss: 0.3287 - val_accuracy: 0.9117\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9634\n",
      "Epoch 00046: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1060 - accuracy: 0.9634 - val_loss: 0.3283 - val_accuracy: 0.9125\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9632\n",
      "Epoch 00047: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 64s 165ms/step - loss: 0.1040 - accuracy: 0.9632 - val_loss: 0.3280 - val_accuracy: 0.9116\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9606\n",
      "Epoch 00048: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 65s 165ms/step - loss: 0.1116 - accuracy: 0.9606 - val_loss: 0.3305 - val_accuracy: 0.9110\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9619\n",
      "Epoch 00049: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 68s 175ms/step - loss: 0.1075 - accuracy: 0.9619 - val_loss: 0.3397 - val_accuracy: 0.9113\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9618\n",
      "Epoch 00050: val_accuracy did not improve from 0.91590\n",
      "390/390 [==============================] - 66s 168ms/step - loss: 0.1080 - accuracy: 0.9618 - val_loss: 0.3298 - val_accuracy: 0.9127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f09409fa3c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen_train.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)//128,epochs=50,validation_data=datagen_test.flow(X_test,y_test),callbacks=[model_callback,csv_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8fPWX6AOptj",
    "outputId": "f19eadcb-f933-45f7-fb9e-4af1ddd8186e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 19ms/step - loss: 0.3158 - accuracy: 0.9159\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.hdf5')\n",
    "test_loss,test_accuracy = model.evaluate(datagen_test.flow(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MjSx_WEOwK0",
    "outputId": "69f4fbf0-c3cd-4405-ed06-0c24e70ea73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss : 0.32\n",
      "test_accuracy : 91.59 %\n"
     ]
    }
   ],
   "source": [
    "print(\"test_loss : {:.2f}\".format(test_loss))\n",
    "print(\"test_accuracy : {:.2f} %\".format(test_accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_on_CIFR_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
